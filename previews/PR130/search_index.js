var documenterSearchIndex = {"docs":
[{"location":"likelihoods/categorical_likelihood/#Categorical-likelihoods","page":"Categorical likelihoods","title":"Categorical likelihoods","text":"","category":"section"},{"location":"likelihoods/categorical_likelihood/#Type","page":"Categorical likelihoods","title":"Type","text":"","category":"section"},{"location":"likelihoods/categorical_likelihood/#MarkovKernels.CategoricalLikelihood","page":"Categorical likelihoods","title":"MarkovKernels.CategoricalLikelihood","text":"CategoricalLikelihood\n\nType for representing a Likelihood function over categories.\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/categorical_likelihood/#Constructor","page":"Categorical likelihoods","title":"Constructor","text":"","category":"section"},{"location":"likelihoods/categorical_likelihood/#MarkovKernels.CategoricalLikelihood-Tuple{Likelihood{<:StochasticMatrix}}","page":"Categorical likelihoods","title":"MarkovKernels.CategoricalLikelihood","text":"CategoricalLikelihood(L::Likelihood{<:StochasticMatrix})\n\nComputes a categorical likelihood from L.\n\n\n\n\n\n","category":"method"},{"location":"likelihoods/categorical_likelihood/#Methods","page":"Categorical likelihoods","title":"Methods","text":"","category":"section"},{"location":"likelihoods/categorical_likelihood/#MarkovKernels.likelihood_vector-Tuple{CategoricalLikelihood}","page":"Categorical likelihoods","title":"MarkovKernels.likelihood_vector","text":"likelihood_vector(L::CategoricalLikelihood)\n\nComputes the vector of likelihood evaluations.\n\n\n\n\n\n","category":"method"},{"location":"likelihoods/likelihood/#Likelihood","page":"Likelihood","title":"Likelihood","text":"","category":"section"},{"location":"likelihoods/likelihood/#Type","page":"Likelihood","title":"Type","text":"","category":"section"},{"location":"likelihoods/likelihood/#MarkovKernels.Likelihood","page":"Likelihood","title":"MarkovKernels.Likelihood","text":"Likelihood{U,V}\n\nType for representing a Likelihood associated with a kernel K(y, x) and a measurement y.\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/likelihood/#Constructors","page":"Likelihood","title":"Constructors","text":"","category":"section"},{"location":"likelihoods/likelihood/#MarkovKernels.Likelihood-Tuple{AbstractMarkovKernel, Any}","page":"Likelihood","title":"MarkovKernels.Likelihood","text":"Likelihood(K::AbstractMarkovKernel, y)\n\nCreates a Likelihood with measurement kernel K and measurement y.\n\n\n\n\n\n","category":"method"},{"location":"likelihoods/likelihood/#Methods","page":"Likelihood","title":"Methods","text":"","category":"section"},{"location":"likelihoods/likelihood/#MarkovKernels.measurement_model-Tuple{Likelihood}","page":"Likelihood","title":"MarkovKernels.measurement_model","text":"measurement_model(L::Likelihood)\n\nComputes the measurement kernel K.\n\n\n\n\n\n","category":"method"},{"location":"likelihoods/likelihood/#MarkovKernels.measurement-Tuple{Likelihood}","page":"Likelihood","title":"MarkovKernels.measurement","text":"measurement(L::Likelihood)\n\nComputes the measurement y\n\n\n\n\n\n","category":"method"},{"location":"kernels/normalkernel/#NormalKernel","page":"NormalKernel","title":"NormalKernel","text":"","category":"section"},{"location":"kernels/normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"The Normal kernel is denoted by","category":"page"},{"location":"kernels/normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"k(ymid x) = mathcalN(y  mu(x)   Sigma(x) )","category":"page"},{"location":"kernels/normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"As with the Normal distributions, the explicit expression on the kernel depends on whether it is real or complex valued.","category":"page"},{"location":"kernels/normalkernel/#Types","page":"NormalKernel","title":"Types","text":"","category":"section"},{"location":"kernels/normalkernel/#MarkovKernels.AbstractNormalKernel","page":"NormalKernel","title":"MarkovKernels.AbstractNormalKernel","text":"AbstractNormalKernel\n\nAbstract type for representing Normal kernels.\n\n\n\n\n\n","category":"type"},{"location":"kernels/normalkernel/#MarkovKernels.NormalKernel","page":"NormalKernel","title":"MarkovKernels.NormalKernel","text":"NormalKernel\n\nStandard mean vector / covariance matrix parametrisation of Normal kernels.\n\n\n\n\n\n","category":"type"},{"location":"kernels/normalkernel/#Type-aliases","page":"NormalKernel","title":"Type aliases","text":"","category":"section"},{"location":"kernels/normalkernel/","page":"NormalKernel","title":"NormalKernel","text":"const HomoskedasticNormalKernel{TM,TC} = NormalKernel{<:Homoskedastic,TM,TC} where {TM,TC} # constant conditional covariance\nconst AffineHomoskedasticNormalKernel{TM,TC} =\n    NormalKernel{<:Homoskedastic,TM,TC} where {TM<:AbstractAffineMap,TC} # affine conditional mean, constant conditional covariance\nconst AffineHeteroskedasticNormalKernel{TM,TC} =\n    NormalKernel{<:Heteroskedastic,TM,TC} where {TM<:AbstractAffineMap,TC} # affine conditional mean, non-constant covariance\nconst NonlinearNormalKernel{TM,TC} = NormalKernel{<:Heteroskedastic,TM,TC} where {TM,TC} # the general, nonlinear case","category":"page"},{"location":"kernels/normalkernel/#Constructors","page":"NormalKernel","title":"Constructors","text":"","category":"section"},{"location":"kernels/normalkernel/#MarkovKernels.NormalKernel-Tuple{Any, Any}","page":"NormalKernel","title":"MarkovKernels.NormalKernel","text":"Normal(μ, Σ)\n\nCreates a Normal kernel with conditional mean and covariance parameters μ and  Σ, respectively.\n\n\n\n\n\n","category":"method"},{"location":"kernels/normalkernel/#Methods","page":"NormalKernel","title":"Methods","text":"","category":"section"},{"location":"kernels/normalkernel/#Statistics.mean-Tuple{NormalKernel}","page":"NormalKernel","title":"Statistics.mean","text":"mean(K::AbstractNormalKernel)\n\nComputes the conditonal mean function of the Normal kernel K. That is, the output is callable.\n\n\n\n\n\n","category":"method"},{"location":"kernels/normalkernel/#Statistics.cov-Tuple{NormalKernel}","page":"NormalKernel","title":"Statistics.cov","text":"cov(K::AbstractNormalKernel)\n\nComputes the conditonal covariance function of the Normal kernel K. That is, the output is callable.\n\n\n\n\n\n","category":"method"},{"location":"kernels/normalkernel/#MarkovKernels.covparam-Tuple{NormalKernel}","page":"NormalKernel","title":"MarkovKernels.covparam","text":"covparam(K::AbstractNormalKernel)\n\nReturns the internal representation of the conditonal covariance matrix of the Normal kernel K. For computing the actual conditional covariance matrix, use cov.\n\n\n\n\n\n","category":"method"},{"location":"kernels/normalkernel/#Base.rand-Tuple{Random.AbstractRNG, AbstractNormalKernel, Union{AbstractVector{T}, T} where T<:Number}","page":"NormalKernel","title":"Base.rand","text":"rand([rng::AbstractRNG], K::AbstractNormalKernel, x::AbstractVector)\n\nSamples a random vector conditionally on x with respect the the Normal kernel K using the random number generator rng.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/internal/#Internal-methods-for-covariance-parameters","page":"Internal","title":"Internal methods for covariance parameters","text":"","category":"section"},{"location":"PSDParametrizations/internal/#MarkovKernels.utrisqrt2utrichol!","page":"Internal","title":"MarkovKernels.utrisqrt2utrichol!","text":"utrisqrt2utrichol!(A::AbstractMatrix)\n\nApplies an in-place orthogonal transform to the upper triangular matrix A, such that the diagonal entries are real and positive, i.e. the resulting matrix is a valid Cholesky factor.   \n\n\n\n\n\n","category":"function"},{"location":"PSDParametrizations/internal/#MarkovKernels.positive_qrwoq!-Tuple{AbstractMatrix}","page":"Internal","title":"MarkovKernels.positive_qrwoq!","text":"positive_qrwoq!(A::AbstractMatrix)\n\nComputes the R factor in the QR decomposition of A, in-place, ensuring that the diagonal entries of R are positive. The returned object is a view of A. \n\n\n\n\n\n","category":"method"},{"location":"likelihoods/flatlikelihood/#Flat-likelihoods","page":"Flat likelihoods","title":"Flat likelihoods","text":"","category":"section"},{"location":"likelihoods/flatlikelihood/","page":"Flat likelihoods","title":"Flat likelihoods","text":"A flat likelihood, L, acts as ideentity under Bayes' rule, that is:","category":"page"},{"location":"likelihoods/flatlikelihood/","page":"Flat likelihoods","title":"Flat likelihoods","text":"D(x) = fracL(x)D(x)int L(x) D(x) dx","category":"page"},{"location":"likelihoods/flatlikelihood/#Type","page":"Flat likelihoods","title":"Type","text":"","category":"section"},{"location":"likelihoods/flatlikelihood/#MarkovKernels.FlatLikelihood","page":"Flat likelihoods","title":"MarkovKernels.FlatLikelihood","text":"FlatLikelihood\n\nType for representing flat likelihoods.\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/likelihood_general/#General","page":"General","title":"General","text":"","category":"section"},{"location":"likelihoods/likelihood_general/#Type","page":"General","title":"Type","text":"","category":"section"},{"location":"likelihoods/likelihood_general/#MarkovKernels.AbstractLikelihood","page":"General","title":"MarkovKernels.AbstractLikelihood","text":"AbstractLikelihood\n\nAbstract type for representing likelihood functions.\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/likelihood_general/#Methods","page":"General","title":"Methods","text":"","category":"section"},{"location":"likelihoods/likelihood_general/#Base.log-Tuple{AbstractLikelihood, Any}","page":"General","title":"Base.log","text":"log(L::AbstractLikelihood, x)\n\nComputes the logarithm of L evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"distributions/probability_vector/#Probability-vector","page":"Probability vector","title":"Probability vector","text":"","category":"section"},{"location":"distributions/probability_vector/#Types","page":"Probability vector","title":"Types","text":"","category":"section"},{"location":"distributions/probability_vector/#MarkovKernels.AbstractProbabilityVector","page":"Probability vector","title":"MarkovKernels.AbstractProbabilityVector","text":"AbstractProbabilityVector{ST}\n\nAbstract type for representing categorical distributions with values ST.\n\n\n\n\n\n","category":"type"},{"location":"distributions/probability_vector/#MarkovKernels.ProbabilityVector","page":"Probability vector","title":"MarkovKernels.ProbabilityVector","text":"ProbabilityVector{T,A}\n\nType for representing categorical distributions with sample_eltype T.\n\n\n\n\n\n","category":"type"},{"location":"distributions/probability_vector/#Constructor","page":"Probability vector","title":"Constructor","text":"","category":"section"},{"location":"distributions/probability_vector/#MarkovKernels.ProbabilityVector-Tuple{AbstractVector}","page":"Probability vector","title":"MarkovKernels.ProbabilityVector","text":"ProbabilityVector(p::AbstractVector)\n\nConstructs a categorical distribution from the vector of probabilities p.\n\n\n\n\n\n","category":"method"},{"location":"distributions/probability_vector/#Methods","page":"Probability vector","title":"Methods","text":"","category":"section"},{"location":"distributions/probability_vector/#MarkovKernels.probability_vector-Tuple{AbstractProbabilityVector}","page":"Probability vector","title":"MarkovKernels.probability_vector","text":"probability_vector(::AbstractProbabilityVector)\n\nComputes the vector of probabilities for each category.\n\n\n\n\n\n","category":"method"},{"location":"distributions/probability_vector/#MarkovKernels.entropy-Tuple{AbstractProbabilityVector}","page":"Probability vector","title":"MarkovKernels.entropy","text":"entropy(C::AbstractProbabilityVector)\n\nComputes the entropy of the categorical distribution C.\n\n\n\n\n\n","category":"method"},{"location":"distributions/probability_vector/#MarkovKernels.kldivergence-Tuple{AbstractProbabilityVector, AbstractProbabilityVector}","page":"Probability vector","title":"MarkovKernels.kldivergence","text":"kldivergence(C1::AbstractProbabilityVector, C2::AbstractProbabilityVector)\n\nComputes the Kullback-Leibler divergence between the categorical distributions C1 and C2.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"MarkovKernels.jl aims to support a wide variety of parametrizations of positive semi-definite matrices. How this is accomplished is explaiend in the following.","category":"page"},{"location":"PSDParametrizations/general/#Types","page":"General","title":"Types","text":"","category":"section"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"Currently, the following types are valid PSD parametrizations (in the sense, the PSDParametriszations methods below have been implemented):","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"Real\nSelfAdjoint\nCholesky","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"Here, the definition of SelfAdjoint is:","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"const RealSymmetric{T,S} = Symmetric{T,S} where {T<:Real,S}\nconst ComplexHermitian{T,S} = Hermitian{T,S} where {T<:Complex,S}\nconst RealDiagonal{T,S} = Diagonal{T,S} where {T<:Real,S}\nconst SelfAdjoint{T,S} =\n    Union{RealSymmetric{T,S},ComplexHermitian{T,S},RealDiagonal{T,S}} where {T,S}","category":"page"},{"location":"PSDParametrizations/general/#PSD-Trait","page":"General","title":"PSD Trait","text":"","category":"section"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"In order to determine, whether a type is a PSDParametrization the following types are defined:","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"abstract type PSDTrait end\nstruct IsPSD <: PSDTrait end\nstruct IsNotPSD <: PSDTrait end","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"A custom typer can opt into being a PSDparametrization by implementing","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"psdcheck(::MyPSDType) = IsPSD()","category":"page"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"which is a promise that the methods of the next section have been implemented.","category":"page"},{"location":"PSDParametrizations/general/#PSDParametrizations-methods","page":"General","title":"PSDParametrizations methods","text":"","category":"section"},{"location":"PSDParametrizations/general/#MarkovKernels.psdcheck-Tuple{Any}","page":"General","title":"MarkovKernels.psdcheck","text":"psdcheck(A)\n\nReturns IsPSD() if A is a PSDParametrization otherwise IsNotPSD()\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#MarkovKernels.convert_psd_eltype-Union{Tuple{T}, Tuple{Type{T}, Any}} where T","page":"General","title":"MarkovKernels.convert_psd_eltype","text":"convert_psd_eltype(::Type{T}, P)\n\nWraps P in a psd paramtrization of eltype T. If P is already a type of psd paramtrization, then just the eltype is converted.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#MarkovKernels.rsqrt-Tuple{Any}","page":"General","title":"MarkovKernels.rsqrt","text":"rsqrt(Σ)\n\nComputes a right square-root of Σ.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#MarkovKernels.lsqrt-Tuple{Any}","page":"General","title":"MarkovKernels.lsqrt","text":"lsqrt(Σ)\n\nComputes a left square-root of Σ.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#MarkovKernels.stein-Tuple{Any, Any, Any}","page":"General","title":"MarkovKernels.stein","text":"stein(Σ, Φ, [Q])\n\nComputes the output of the stein  operator\n\nΣ ↦ Φ * Σ * Φ' + Q.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#MarkovKernels.schur_reduce-Tuple{Any, Any, Any}","page":"General","title":"MarkovKernels.schur_reduce","text":"schur_reduce(Π, C, [R])\n\nComputes the tuple (S, K, Σ) associated with the following (block) Schur reduction:\n\n[C*Π*C'+R C*Π; Π*C' Π] = [0 0; 0 Σ] + [I; K]*(C*Π*C' + R)*[I; K]'\n\nIn terms of Kalman filtering, Π is the predictive covariance, C the measurement matrix, and R the measurement covariance, then S is the marginal measurement covariance, K is the Kalman gain, and Σ is the filtering covariance.\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#SelfAdjoint-methods","page":"General","title":"SelfAdjoint methods","text":"","category":"section"},{"location":"PSDParametrizations/general/","page":"General","title":"General","text":"Additionally, the following methods are defined for SelfAdjoint:","category":"page"},{"location":"PSDParametrizations/general/#MarkovKernels.selfadjoint!-Tuple{Number}","page":"General","title":"MarkovKernels.selfadjoint!","text":"selfadjoint!(A)\n\nComputes the self-adjoint part of A, in-place, and wraps it in an appropriate self-adjoint wrapper type (i.e. Symemtric / Hermitian).\n\n\n\n\n\n","category":"method"},{"location":"PSDParametrizations/general/#MarkovKernels.selfadjoint-Tuple{Any}","page":"General","title":"MarkovKernels.selfadjoint","text":"selfadjoint(A)\n\nComputes the self-adjoint part of A and wraps it in an appropriate self-adjoint wrapper type (i.e. Symemtric / Hermitian).\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Composition","page":"Binary operators","title":"Composition","text":"","category":"section"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"Given Markov kernels k_2(yz) and k_1(zx), composition is a binary operator producing a third kernel k_3(yx) according to","category":"page"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"k_3(yx) = int k_2(yx) k_1(zx) mathrmd z","category":"page"},{"location":"binary_operators/#MarkovKernels.compose-Tuple{AbstractMarkovKernel, AbstractMarkovKernel}","page":"Binary operators","title":"MarkovKernels.compose","text":"compose(K2::AbstractMarkovKernel, K1::AbstractMarkovKernel)\n\nComputes K3, the composition of K2 ∘ K1 i.e.,\n\nK3(y,x) = ∫ K2(y,z) K1(z,x) dz.\n\nSee also ∘\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Base.:∘-Tuple{AbstractMarkovKernel, AbstractMarkovKernel}","page":"Binary operators","title":"Base.:∘","text":"∘(K2::AbstractMarkovKernel, K1::AbstractMarkovKernel)\n\nComputes K3, the composition of K2 ∘ K1 i.e.,\n\nK3(y,x) = ∫ K2(y,z) K1(z,x) dz.\n\nSee also compose\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"Additionally, given likelihoods l_1(x) and l_2(x), composition is a binary operator producing a third likelihood l_3(x) according to","category":"page"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"l_3(x) = l_2(x) l_1(x)","category":"page"},{"location":"binary_operators/#MarkovKernels.compose-Tuple{AbstractLikelihood, AbstractLikelihood}","page":"Binary operators","title":"MarkovKernels.compose","text":"compose(L2::AbstractLikelihood, L1::AbstractLiklelihood)\n\nComputes L3, the composition of L2 ∘ L1 i.e.,\n\nL3(x) = L1(x) * L2(x)\n\nSee also ∘\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Base.:∘-Tuple{AbstractLikelihood, AbstractLikelihood}","page":"Binary operators","title":"Base.:∘","text":"∘(K2::AbstractLikelihood, K1::AbstractLikelihood)\n\nComputes L3, the composition of L2 ∘ L1 i.e.,\n\nL3(x) = L1(x) * L2(x)\n\nSee also compose\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Algebra","page":"Binary operators","title":"Algebra","text":"","category":"section"},{"location":"binary_operators/#Base.:+-Tuple{AbstractDistribution, Union{AbstractVector{T}, T} where T<:Number}","page":"Binary operators","title":"Base.:+","text":"+(v::AbstractNumOrVec, D::AbstractDistribution) +(D::AbstractDistribution, v::AbstractNumOrVec)\n\nComputes a translation of D by v, i.e. if x ∼ D then x + v ∼ D + v.\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Base.:--Tuple{Normal}","page":"Binary operators","title":"Base.:-","text":"-(D::AbstractDistribution)\n\nComputes the image distribution of D under negation, i.e. if x ∼ D then -x ∼ -D.\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Base.:--Tuple{Union{AbstractVector{T}, T} where T<:Number, AbstractDistribution}","page":"Binary operators","title":"Base.:-","text":"-(v::AbstractNumOrVec, D::AbstractDistribution)\n\nEquivalent to +(v, -D).\n\n-(D::AbstractDistribution, v::AbstractNumOrVec)\n\nEquivalent to +(D, -v).\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Base.:*-Tuple{Any, AbstractDistribution}","page":"Binary operators","title":"Base.:*","text":"*(C, D::AbstractDistribution)\n\nEquivalent to forward_operator(D, DiracKernel(LinearMap(C))).\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Forward-operator","page":"Binary operators","title":"Forward operator","text":"","category":"section"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"A Markov kernel k(y x) defines an operator that maps a distribution pi(x) according to","category":"page"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"int k(y x) pi(x) mathrmd x","category":"page"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"This is the so-called forward operator.","category":"page"},{"location":"binary_operators/#MarkovKernels.forward_operator-Tuple{AbstractMarkovKernel, Any}","page":"Binary operators","title":"MarkovKernels.forward_operator","text":"forward_operator(k::AbstractMarkovKernel, d)\n\nComputes the output of the forward operator associated with k, gvien the input d, i.e.\n\n k(y x) d(x) dx\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Bayes'-rule-and-invert","page":"Binary operators","title":"Bayes' rule & invert","text":"","category":"section"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"Given a distribution pi(x) and a Markov kernel k(yx), invert is a binary operator producing a new distribution m(y) and a new Markov kernel p(x  y) according to","category":"page"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"pi(x) k(yx) = m(y) p(xy)","category":"page"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"The related binary operator, Bayes' rule also evalautes the output of invert at some measurement y. That is, given a measurmeent y, m evaluated at y is the marginal likelihood and p evaluated at y is the conditional distribution of x given y.","category":"page"},{"location":"binary_operators/#MarkovKernels.invert-Tuple{AbstractDistribution, AbstractMarkovKernel}","page":"Binary operators","title":"MarkovKernels.invert","text":"invert(D::AbstractDistribution, K::AbstractMarkovKernel)\n\nComputes a new distribution and Markov kernel such that\n\nDout(y) = ∫ K(y, x) D(x) dx, and Kout(x, y) = K(y, x) * D(x) / Dout(y)\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#MarkovKernels.posterior_and_loglike-Tuple{AbstractDistribution, AbstractMarkovKernel, Any}","page":"Binary operators","title":"MarkovKernels.posterior_and_loglike","text":"posterior_and_loglike(D::AbstractDistribution, K::AbstractMarkovKernel, y)\n\nComputes the conditional distribution C and the marginal log-likelihood ℓ associated with the prior distribution D, measurement kernel K, and measurement y.\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#MarkovKernels.posterior_and_loglike-Tuple{AbstractDistribution, AbstractLikelihood}","page":"Binary operators","title":"MarkovKernels.posterior_and_loglike","text":"posterior_and_loglike(D::AbstractDistribution, L::AbstractLikelihood)\n\nComputes the conditional distribution C and the marginal log-likelihood ℓ associated with the prior distribution D and the likelihood L.\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#MarkovKernels.posterior-Tuple{AbstractDistribution, AbstractMarkovKernel, Any}","page":"Binary operators","title":"MarkovKernels.posterior","text":"posterior(D::AbstractDistribution, K::AbstractMarkovKernel, y)\n\nComputes the conditional distribution C associated with the prior distribution D, measurement kernel K, and measurement y.\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#MarkovKernels.posterior-Tuple{AbstractDistribution, AbstractLikelihood}","page":"Binary operators","title":"MarkovKernels.posterior","text":"posterior(D::AbstractDistribution, L::AbstractLikelihood)\n\nComputes the conditional distribution C associated with the prior distribution D and the likelihood L.\n\n\n\n\n\n","category":"method"},{"location":"binary_operators/#Doob's-h-transform","page":"Binary operators","title":"Doob's h-transform","text":"","category":"section"},{"location":"binary_operators/","page":"Binary operators","title":"Binary operators","text":"Given a Markov kernel k(y x) and a likelihood function h(y), computes a new Markov kernel f(y x) and new likelihood function g(x)","category":"page"},{"location":"binary_operators/#MarkovKernels.htransform_and_likelihood-Tuple{AbstractMarkovKernel, AbstractLikelihood}","page":"Binary operators","title":"MarkovKernels.htransform_and_likelihood","text":"htransform_and_likelihood(K::AbstractMarkovKernel, L::AbstractLikelihood)\n\nComputes a Markov kernel Kout and Likelihood Lout such that\n\nLout(z) = ∫ L(x) K(x, z) dx, and Kout(x, z) = L(x) K(x, z) / Lout(z)\n\n\n\n\n\n","category":"method"},{"location":"kernels/kernel_general/#General","page":"General","title":"General","text":"","category":"section"},{"location":"kernels/kernel_general/#Type","page":"General","title":"Type","text":"","category":"section"},{"location":"kernels/kernel_general/#MarkovKernels.AbstractMarkovKernel","page":"General","title":"MarkovKernels.AbstractMarkovKernel","text":"AbstractMarkovKernel\n\nAbstract type for representing Markov kernels.\n\n\n\n\n\n","category":"type"},{"location":"kernels/kernel_general/#Methods","page":"General","title":"Methods","text":"","category":"section"},{"location":"kernels/kernel_general/#MarkovKernels.condition-Tuple{AbstractMarkovKernel, Any}","page":"General","title":"MarkovKernels.condition","text":"condition(K::AbstractMarkovKernel, x)\n\nComputes the distribution retrieved from evaluating K at x.\n\n\n\n\n\n","category":"method"},{"location":"distributions/dirac/#Dirac","page":"Dirac","title":"Dirac","text":"","category":"section"},{"location":"distributions/dirac/","page":"Dirac","title":"Dirac","text":"The Dirac distribution with parameter mu is a distribution putting all probabiltiy mass on mu. It is denoted by","category":"page"},{"location":"distributions/dirac/","page":"Dirac","title":"Dirac","text":"delta(x -mu)","category":"page"},{"location":"distributions/dirac/#Types","page":"Dirac","title":"Types","text":"","category":"section"},{"location":"distributions/dirac/#MarkovKernels.AbstractDirac","page":"Dirac","title":"MarkovKernels.AbstractDirac","text":"AbstractDirac{ST}\n\nAbstract type for representing Dirac distributions taking values in ST.\n\n\n\n\n\n","category":"type"},{"location":"distributions/dirac/#MarkovKernels.Dirac","page":"Dirac","title":"MarkovKernels.Dirac","text":"Dirac\n\nType for representing Dirac distributions with sample_eltype ST.\n\n\n\n\n\n","category":"type"},{"location":"distributions/dirac/#Constructors","page":"Dirac","title":"Constructors","text":"","category":"section"},{"location":"distributions/dirac/#MarkovKernels.Dirac-Tuple{AbstractVector}","page":"Dirac","title":"MarkovKernels.Dirac","text":"Dirac(μ)\n\nCreates a Dirac distribution with mean μ.\n\n\n\n\n\n","category":"method"},{"location":"distributions/dirac/#Methods","page":"Dirac","title":"Methods","text":"","category":"section"},{"location":"distributions/dirac/#MarkovKernels.dim-Tuple{Dirac}","page":"Dirac","title":"MarkovKernels.dim","text":"dim(D::AbstractDirac)\n\nReturns the dimension of the Dirac distribution D.\n\n\n\n\n\n","category":"method"},{"location":"distributions/dirac/#Statistics.mean-Tuple{Dirac}","page":"Dirac","title":"Statistics.mean","text":"mean(D::AbstractDirac)\n\nComputes the mean vector of the Dirac distribution D.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/gauss_markov_regression/#Sampling-and-inference-in-Gauss-Markov-models","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Gauss-Markov realizable signal, s_t, is a signal that can be constructed as transform of a Gauss-Markov process:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"beginaligned\nx_0 sim mathcalN(mu_0 Sigma_0) \nx_t mid x_u sim mathcalN( Phi_t u x_u Q_t u) quad u  t \ns_t mid x_t sim delta(cdotp - C x_t)\nendaligned","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"If x_t is equal in distribution, to the solution of a linear time-invariant stochastic differential equation, then there matrices A and B such that the transition matrix, Phi, and transition covariance Q are given by","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"beginaligned\nPhi_t u = e^A (t - u)\nQ_t u = sqrtt-u int_0^1 e^A(t-u)z B B^* e^A^*(t-u)z mathrmd z\nendaligned","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The signal is frequently only available through noisy observations:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"y_t_k mid s_t_k sim mathcalN(s_t_k R) quad k = 1 ldots K","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"This tutorial describes how to use MarkovKernels.jl to:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Sample Gauss-Markov realizable signals\nSample observations of Gauss-Markov realizable signals\nCompute the path-posterior of the latent state x on a grid that includes all the observations\nCompute time-marginals of the latent state x and the signal s","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"using MarkovKernels\nusing FiniteHorizonGramians\nusing Random, LinearAlgebra\nimport Plots\nrng = Random.Xoshiro(19910215)\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/#Implementing-samplers-for-Gauss-Markov-realizable-signals","page":"Sampling and inference in Gauss-Markov models","title":"Implementing samplers for Gauss-Markov realizable signals","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The code for sampling from a Gauss-Markov realizable signal is exactly the same as in the hidden Markov model tutorial. For completeness, the code is given below.","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"function sample(rng::AbstractRNG, init, fw_kernels)\n    x = rand(rng, init)\n    n = length(fw_kernels) + 1\n    xs = Vector{typeof(x)}(undef, n)\n    xs[begin] = x\n\n    for (m, fw_kernel) in pairs(fw_kernels)\n        x = rand(rng, condition(fw_kernel, x))\n        xs[begin+m] = x\n    end\n    return xs\nend\n\nfunction sample(rng::AbstractRNG, init, fw_kernels, obs_kernels)\n    # sample initial values\n    x = rand(rng, init)\n    y = rand(rng, first(obs_kernels), x)\n\n    # allocate output\n    n = length(obs_kernels)\n    xs = Vector{typeof(x)}(undef, n)\n    ys = Vector{typeof(y)}(undef, n)\n\n    xs[begin] = x\n    ys[begin] = y\n\n    for (m, fw_kernel) in pairs(fw_kernels)\n        obs_kernel = obs_kernels[begin+m]\n        x = rand(rng, condition(fw_kernel, x))\n        y = rand(rng, condition(obs_kernel, x))\n        xs[begin+m] = x\n        ys[begin+m] = y\n    end\n    return xs, ys\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/#Defining-a-Gauss-Markov-realizable-signal-and-computing-transition-kernels","page":"Sampling and inference in Gauss-Markov models","title":"Defining a Gauss-Markov realizable signal and computing transition kernels","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Implementation of transition kernel computation in Cholesky parametrization for continuous-time Gauss-Markov processes may be done by using FiniteHorizonGramians.jl like so:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"function transition_kernel(A, B, dt)\n    alg = FiniteHorizonGramians.ExpAndGram{Float64,13}()\n    Φ, U = FiniteHorizonGramians.exp_and_gram_chol(A, B, dt, alg)\n    Φ = LinearMap(Φ)\n    Q = Cholesky(UpperTriangular(U))\n    return NormalKernel(Φ, Q)\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Definition of some  parametrized continuous-time Gauss-Markov realizable process:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"function gauss_markov_realizable_model(λ, σ, p)\nA = λ * (I - 2 * tril(ones(p, p)))\nB = sqrt(2λ) * ones(p, 1)\nCadj =\n    σ * factorial(p - 1) / sqrt(factorial(2(p - 1))) .* binomial.(p - 1, 0:p-1) .*\n    (-1) .^ (0:p-1)\nC = adjoint(Cadj)\nreturn A, B, C\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Computing initial density, transition kernels, output kernel, and observation kernels:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"# time grid\nn = 2^9 + 1\nT = 20.0\nts = LinRange(zero(T), T, n)\n\n# model parameters\nλ = 2.0\nσ = 1.0\np = 4\n\n# model matrices\nA, B, C = gauss_markov_realizable_model(λ, σ, p)\n\n\ninit = Normal(zeros(p), cholesky(diagm(0 => ones(p))))\nforward_kernels = [transition_kernel(A, B, dt) for dt in diff(ts)]\noutput_kernel = DiracKernel(LinearMap(C))\noutput_kernels = fill(output_kernel, n)\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Sampling the latent state and the signal:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"xs, ss = sample(rng, init, forward_kernels, output_kernels)\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Defining observation kernel and sampling observations:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"R = 0.1\nobservation_kernel = NormalKernel(LinearMap(one(R)), R)\nys = [rand(rng, condition(observation_kernel, s)) for s in ss]\n\nobs_idx = [mod(n, 2^3) == 1 for n in eachindex(ts)]\nobs_ts = ts[obs_idx]\nys = Union{eltype(ys), Missing}[obs_idx[n] ? ys[n] : missing for n in eachindex(obs_idx)]\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/#Plotting-the-realization","page":"Sampling and inference in Gauss-Markov models","title":"Plotting the realization","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"gmr_plt = Plots.plot(\n    ts,\n    ss,\n    color = \"black\",\n    label = \"\",\n    xlabel = \"t\"\n)\nPlots.scatter!(\n    gmr_plt,\n    obs_ts,\n    filter(!ismissing, ys),\n    color = \"red\",\n    label = \"\"\n)\ngmr_plt","category":"page"},{"location":"tutorials/gauss_markov_regression/#Implementing-the-forward-algorithm","page":"Sampling and inference in Gauss-Markov models","title":"Implementing the forward algorithm","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The forward algorithm operates on a sequence of a posteriori terminal distributions (so-called filtering distributions):","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"p(x_t_m mid y_t_0t_m)","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The algorithm first initializes by:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"beginaligned\np(y_t_0) = int p(y_t_0 mid x_t_0) p(x_t_0) mathrmd x_t_0 \np(x_0 mid y_0) = p(y_t_0 mid x_t_0) p(x_t_0)  p(y_t_0)\nendaligned","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"These two equations are implemented by posterior_and_loglike. The algorithm then computes a sequence of filtering distributions, reverse-time transition kernels, and accumulates the log-likelihood of the observations according to the following forward recursion:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"beginaligned\np(x_t_m mid y_0t_m-1) = int p(x_t_m mid x_t_m-1) p(x_t_m-1 mid y_t_0t_m-1) mathrmd x_t_m-1 \np(x_t_m-1 mid x_t_m-1 y_t_0t_n) = p(x_t_m mid x_t_m-1) p(x_t_m-1 mid y_t_0t_m-1)  p(x_t_m mid y_t_0t_m-1) \np(y_t_m mid y_t_0t_m-1) = int p(y_t_m mid x_t_m) p(x_t_m mid y_t_0t_m-1) mathrmd x_t_m \np(x_t_m mid y_t_0t_m) = p(y_t_m mid x_t_m) p(x_t_m mid y_t_0t_m-1)  p(y_t_m mid y_t_0t_m-1) \nlog p(y_t_0t_m) = log p(y_t_0t_m-1) + log p(y_t_m mid y_t_0t_m-1)\nendaligned","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The first two equations are implemented by invert and the next two equations are, again, implemented by posterior_and_loglike. The last equation is simply a cumulative sum of quantities already computed. Using MarkovKernels.jl, the code might look something like the following:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"function forward_recursion(init, forward_kernels, likelihoods)\n    like = first(likelihoods)\n    filt, loglike = posterior_and_loglike(init, like)\n    KT = Base.promote_op(last ∘ invert, typeof(filt), eltype(forward_kernels))\n    backward_kernels = Vector{KT}(undef, length(forward_kernels))\n    for m in eachindex(forward_kernels)\n        fw_kernel = forward_kernels[m]\n        pred, bw_kernel = invert(filt, fw_kernel)\n\n        like = likelihoods[m+1]\n        filt, ll_incr = posterior_and_loglike(pred, like)\n        loglike = loglike + ll_incr\n        backward_kernels[m] = bw_kernel\n    end\n    return backward_kernels, filt, loglike\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/#Computing-a-reverse-time-Markov-a-posteriori-path-distribution","page":"Sampling and inference in Gauss-Markov models","title":"Computing a reverse-time Markov a posteriori path-distribution","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"In order to compute the posterior, likelihood functions for the latent state based on the observations. Some observations are of type Missing but Likelihood{<:AbstractMarkovKernel,<:Missing} is interpreted as a likelihood of type FlatLikelihood, which turns posterior_and_loglike into a kind of identity mapping.","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"likelihoods = [Likelihood(compose(observation_kernel, output_kernel), y) for y in ys]\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The reverse-time posterior is now computed by:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"backward_kernels, term, loglike = forward_recursion(init, forward_kernels, likelihoods)\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/#Computing-a-posteriori-time-marginals","page":"Sampling and inference in Gauss-Markov models","title":"Computing a posteriori time-marginals","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The a posteriori time marginals may be computed according to the following backward recursion:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"p(x_t_m-1 mid y_t_0t_n) = int p(x_t_m-1 mid x_t_m y_t_0t_n)\np(x_t_m mid y_t_0t_n) mathrmd x_t_m","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"This equation is implemented by forward_operator. Using MarkovKernels.jl, the code might look something like the following:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"function reverse_time_marginals(term, bw_kernels)\n    dist = term\n    dists = Vector{typeof(init)}(undef, length(bw_kernels)+1)\n    dists[end] = dist\n    for (m, kernel) in pairs(reverse(bw_kernels))\n        dist = forward_operator(kernel, dist)\n        dists[end-m] = dist\n    end\n    return dists\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"The time-marginals are then computed like so:","category":"page"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"post_state_dists = reverse_time_marginals(term, backward_kernels)\npost_output_dists = [forward_operator(output_kernel, dist) for dist in post_state_dists]\n\nnothing # hide","category":"page"},{"location":"tutorials/gauss_markov_regression/#Plotting-the-a-posteriori-time-marginals-and-samples","page":"Sampling and inference in Gauss-Markov models","title":"Plotting the a posteriori time marginals and samples","text":"","category":"section"},{"location":"tutorials/gauss_markov_regression/","page":"Sampling and inference in Gauss-Markov models","title":"Sampling and inference in Gauss-Markov models","text":"Plots.plot!(\n    gmr_plt,\n    ts,\n    post_output_dists,\n    color = \"blue\",\n    label = \"\",\n)\n\nnsample = 5\nfor _ in 1:nsample\n    # sample assumes an initial distribution and a series of forward kenrels\n    # so the backward_kernels need to be wrapped in reverse\n    # giving the signal in reverse order\n    _, ss_post = sample(rng, term, reverse(backward_kernels), output_kernels)\n    # reverse the signal path so that it is in the correct order\n    ss_post = reverse(ss_post)\n    Plots.plot!(\n        gmr_plt,\n        ts,\n        ss_post,\n        color = \"blue\",\n        label = \"\",\n    )\nend\ngmr_plt","category":"page"},{"location":"kernels/dirackernel/#DiracKernel","page":"DiracKernel","title":"DiracKernel","text":"","category":"section"},{"location":"kernels/dirackernel/","page":"DiracKernel","title":"DiracKernel","text":"The Dirac kernel with conditional mean prameter  mu is denotd by","category":"page"},{"location":"kernels/dirackernel/","page":"DiracKernel","title":"DiracKernel","text":"k(ymid x) = delta(y - mu(x))","category":"page"},{"location":"kernels/dirackernel/#Types","page":"DiracKernel","title":"Types","text":"","category":"section"},{"location":"kernels/dirackernel/#MarkovKernels.AbstractDiracKernel","page":"DiracKernel","title":"MarkovKernels.AbstractDiracKernel","text":"AbstractDiracKernel\n\nAbstract type for representing Dirac kernels.\n\n\n\n\n\n","category":"type"},{"location":"kernels/dirackernel/#MarkovKernels.DiracKernel","page":"DiracKernel","title":"MarkovKernels.DiracKernel","text":"DiracKernel\n\nType for representing Dirac kernels K(y,x) = δ(y - μ(x)).\n\n\n\n\n\n","category":"type"},{"location":"kernels/dirackernel/#MarkovKernels.IdentityKernel","page":"DiracKernel","title":"MarkovKernels.IdentityKernel","text":"IdentityKernel\n\nStruct for representing kernels that act like identity under marginalization.\n\n\n\n\n\n","category":"type"},{"location":"kernels/dirackernel/#Type-aliases","page":"DiracKernel","title":"Type aliases","text":"","category":"section"},{"location":"kernels/dirackernel/","page":"DiracKernel","title":"DiracKernel","text":"const AffineDiracKernel{T} = DiracKernel{T,<:AbstractAffineMap}","category":"page"},{"location":"kernels/dirackernel/#Methods","page":"DiracKernel","title":"Methods","text":"","category":"section"},{"location":"kernels/dirackernel/#Statistics.mean-Tuple{DiracKernel}","page":"DiracKernel","title":"Statistics.mean","text":"mean(K::AbstractDiracKernel)\n\nComputes the conditonal mean function of the Dirac kernel K. That is, the output is callable.\n\n\n\n\n\n","category":"method"},{"location":"kernels/dirackernel/#Base.rand-Tuple{Random.AbstractRNG, AbstractDiracKernel, Union{AbstractVector{T}, T} where T<:Number}","page":"DiracKernel","title":"Base.rand","text":"rand([rng::AbstractRNG], K::AbstractDiracKernel, x::AbstractVector)\n\nComputes a random vector conditionally on x with respect the the Dirac kernel K using the random number generator RNG. Equivalent to mean(K)(x).\n\n\n\n\n\n","category":"method"},{"location":"kernels/stochasticmatrix/#StochasticMatrix","page":"StochasticMatrix","title":"StochasticMatrix","text":"","category":"section"},{"location":"kernels/stochasticmatrix/#Types","page":"StochasticMatrix","title":"Types","text":"","category":"section"},{"location":"kernels/stochasticmatrix/#MarkovKernels.AbstractStochasticMatrix","page":"StochasticMatrix","title":"MarkovKernels.AbstractStochasticMatrix","text":"AbstractStochasticMatrix\n\nAbstract type for representing stochastic matrices.\n\n\n\n\n\n","category":"type"},{"location":"kernels/stochasticmatrix/#MarkovKernels.StochasticMatrix","page":"StochasticMatrix","title":"MarkovKernels.StochasticMatrix","text":"StochasticMatrix\n\nType for representing stochastic matrices.\n\n\n\n\n\n","category":"type"},{"location":"kernels/stochasticmatrix/#Constructor","page":"StochasticMatrix","title":"Constructor","text":"","category":"section"},{"location":"kernels/stochasticmatrix/#MarkovKernels.StochasticMatrix-Tuple{AbstractMatrix}","page":"StochasticMatrix","title":"MarkovKernels.StochasticMatrix","text":"StochasticMatrix(P::AbstractMatrix)\n\nConstructs a stochastic matrix from the matrix of transition probabilities P.\n\n\n\n\n\n","category":"method"},{"location":"kernels/stochasticmatrix/#Methods","page":"StochasticMatrix","title":"Methods","text":"","category":"section"},{"location":"kernels/stochasticmatrix/#MarkovKernels.probability_matrix-Tuple{AbstractStochasticMatrix}","page":"StochasticMatrix","title":"MarkovKernels.probability_matrix","text":"probability_vector(::AbstractCategorical)\n\nComputes the matrix of transition probabilities.\n\n\n\n\n\n","category":"method"},{"location":"kernels/stochasticmatrix/#Base.rand-Tuple{Random.AbstractRNG, AbstractStochasticMatrix, Int64}","page":"StochasticMatrix","title":"Base.rand","text":"rand([rng::AbstractRNG], K::AbstractStochasticMatrix, x)\n\nSamples a random vector conditionally on x with respect the the stochastic matrix K using the random number generator rng.\n\n\n\n\n\n","category":"method"},{"location":"distributions/distribution_general/#General","page":"General","title":"General","text":"","category":"section"},{"location":"distributions/distribution_general/#Type","page":"General","title":"Type","text":"","category":"section"},{"location":"distributions/distribution_general/#MarkovKernels.AbstractDistribution","page":"General","title":"MarkovKernels.AbstractDistribution","text":"AbstractDistribution{ST}\n\nAbstract type for representing distributions with samples of type ST.\n\n\n\n\n\n","category":"type"},{"location":"distributions/distribution_general/#Type-information","page":"General","title":"Type information","text":"","category":"section"},{"location":"distributions/distribution_general/#MarkovKernels.sample_type-Tuple{AbstractDistribution}","page":"General","title":"MarkovKernels.sample_type","text":"sample_type(D::AbstractDistribution)\n\nComputes the type of samples from D, e.g. same as typeof(rand(D)).\n\n\n\n\n\n","category":"method"},{"location":"distributions/distribution_general/#MarkovKernels.sample_eltype-Tuple{AbstractDistribution}","page":"General","title":"MarkovKernels.sample_eltype","text":"sample_eltype(D::AbstractDistribution)\n\nComputes the eltype of samples from D, e.g. same as eltype(rand(D)).\n\n\n\n\n\n","category":"method"},{"location":"distributions/distribution_general/#Probability-densities","page":"General","title":"Probability densities","text":"","category":"section"},{"location":"distributions/distribution_general/#MarkovKernels.logpdf-Tuple{AbstractDistribution, Any}","page":"General","title":"MarkovKernels.logpdf","text":"logpdf(D::AbstractDistribution, x)\n\nComputes the logarithm of the probabilidty density of D, evaluated at x.\n\n\n\n\n\n","category":"method"},{"location":"distributions/distribution_general/#Sampling","page":"General","title":"Sampling","text":"","category":"section"},{"location":"distributions/distribution_general/#Base.rand-Tuple{Random.AbstractRNG, AbstractDistribution}","page":"General","title":"Base.rand","text":"rand([rng], D::AbstractDistribution)\n\nDraws one sample from D.\n\n\n\n\n\n","category":"method"},{"location":"likelihoods/logquadratic/#Log-quadratic-likelihoods","page":"Log-quadratic likelihoods","title":"Log-quadratic likelihoods","text":"","category":"section"},{"location":"likelihoods/logquadratic/#Type","page":"Log-quadratic likelihoods","title":"Type","text":"","category":"section"},{"location":"likelihoods/logquadratic/#MarkovKernels.LogQuadraticLikelihood","page":"Log-quadratic likelihoods","title":"MarkovKernels.LogQuadraticLikelihood","text":"LogQuadraticLikelihood\n\nType for representing log-quadratic likelihoods.\n\n\n\n\n\n","category":"type"},{"location":"likelihoods/logquadratic/#Constructor","page":"Log-quadratic likelihoods","title":"Constructor","text":"","category":"section"},{"location":"likelihoods/logquadratic/#MarkovKernels.LogQuadraticLikelihood-Tuple{Likelihood{<:NormalKernel{<:Homoskedastic, TM, TC} where {TM<:AbstractAffineMap, TC}}}","page":"Log-quadratic likelihoods","title":"MarkovKernels.LogQuadraticLikelihood","text":"LogQuadraticLikelihood(L::Likelihood{<:AffineHomoskedasticNormalKernel})\n\nComputes a log-quadratic likelihood from L.\n\n\n\n\n\n","category":"method"},{"location":"tutorials/hidden_markov_model/#Sampling-and-inference-in-Hidden-Markov-models","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"A finite state hidden Markov model is specified by an initial distribution, a sequence of transition probabilities, and a sequence of observation probabilities:","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"beginaligned\nP(x_0 = i) \nP(x_t = i mid x_t-1 = j) quad t = 1 ldots T   \nP(y_t = i mid x_t = j) quad t = 0 1 ldots T\nendaligned","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"This tutorial describes how to use MarkovKernels.jl to:","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"Sample from a (finite state) hidden Markov model\nCompute the a posteriori distribution of the hidden sequence using the backward recursion","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"using MarkovKernels\nusing Random, LinearAlgebra\nimport Plots\nrng = Random.Xoshiro(19910215)\n\nnothing # hide","category":"page"},{"location":"tutorials/hidden_markov_model/#Implementing-samplers-for-finite-state-(hidden)-Markov-models","page":"Sampling and inference in Hidden Markov models","title":"Implementing samplers for finite state (hidden) Markov models","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"function sample(rng::AbstractRNG, init, fw_kernels)\n    x = rand(rng, init)\n    n = length(fw_kernels) + 1\n    xs = Vector{typeof(x)}(undef, n)\n    xs[begin] = x\n\n    for (m, fw_kernel) in pairs(fw_kernels)\n        x = rand(rng, condition(fw_kernel, x))\n        xs[begin+m] = x\n    end\n    return xs\nend\n\nfunction sample(rng::AbstractRNG, init, fw_kernels, obs_kernels)\n    # sample initial values\n    x = rand(rng, init)\n    y = rand(rng, first(obs_kernels), x)\n\n    # allocate output\n    n = length(obs_kernels)\n    xs = Vector{typeof(x)}(undef, n)\n    ys = Vector{typeof(y)}(undef, n)\n\n    xs[begin] = x\n    ys[begin] = y\n\n    for (m, fw_kernel) in pairs(fw_kernels)\n        obs_kernel = obs_kernels[begin+m]\n        x = rand(rng, condition(fw_kernel, x))\n        y = rand(rng, condition(obs_kernel, x))\n        xs[begin+m] = x\n        ys[begin+m] = y\n    end\n    return xs, ys\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/hidden_markov_model/#Defining-a-finite-state-hidden-Markov-model-and-sampling-it","page":"Sampling and inference in Hidden Markov models","title":"Defining a finite state hidden Markov model and sampling it","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"# number of possible hidden states and number of possible observation states\nm, n = 10, 10\n\n# probability vector of initial distribution\ninit = ProbabilityVector(ones(m))\n\n# transition probabilities\nPxx = Matrix(Tridiagonal(ones(m - 1), 5 * ones(m), ones(m - 1)))\nKxx = StochasticMatrix(Pxx)\n\n# observation probabilites\nPyx = (ones(m, m) - I)\nKyx = StochasticMatrix(Pyx)\n\nT = 2^8 + 1\nfw_kernels = fill(Kxx, T - 1)\nobs_kernels = fill(Kyx, T)\n\n# sample hidden and observed states\nxs, ys = sample(rng, init, fw_kernels, obs_kernels)\nnothing # hide","category":"page"},{"location":"tutorials/hidden_markov_model/#Plotting-the-realization","page":"Sampling and inference in Hidden Markov models","title":"Plotting the realization","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"hmm_plt = Plots.scatter(\n    layout = (1, 2)\n)\nPlots.scatter!(\n    hmm_plt,\n    eachindex(xs),\n    xs,\n    color = \"black\",\n    subplot = 1,\n    title = \"hidden states\",\n    label = \"\",\n)\nPlots.scatter!(\n    hmm_plt,\n    eachindex(ys),\n    ys,\n    color = \"red\",\n    subplot = 2,\n    title = \"observation states\",\n    label = \"\",\n)","category":"page"},{"location":"tutorials/hidden_markov_model/#Implementing-the-backward-algorithms","page":"Sampling and inference in Hidden Markov models","title":"Implementing the backward algorithms","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"The backward algorith operates on the sequence of likelihoods of future observations:","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"h_tT mid s(x) = P(y_tT mid x_s = x)","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"It computes an a posteriori initial distribution, a sequence of a posteriori transition probabilities, and a log-likelihood of the observations, via a backward recursion. The recursion is given by:","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"beginaligned\nh_tTmid t-1(z) = sum_x h_tT mid t(x) P(x_t = x mid x_t-1 = z)  \nP(x_t = x mid x_t-1 = z y_0T) = h_tT mid t(x) P(x_t = x mid x_t-1 = z)  h_tTmid t-1(z) \nh_t-1T mid t-1(x) = h_tTmid t-1(x) h_t-1 mid t-1(x)\nendaligned","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"The first two equations are implemented by htransform_and_likelihood and the last equation is implemented by compose. The algorithm terminates by computing the a posteriori initial distribution and the log-likelihood of the observations:","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"beginaligned\nlog P(y_0T) =  log Big(sum_x h_0T mid 0(x) P(x_0 = x) Big) \nP(x_0 = x mid y_0T) = h_0T mid 0(x) P(x_0 = x)  P(y_0T)\nendaligned","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"These equations are implemented by posterior_and_loglike. Using MarkovKernels.jl, the code might look something like the following:","category":"page"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"function backward_recursion(init, forward_kernels, likelihoods)\n    h = last(likelihoods)\n    KT = Base.promote_op(first ∘ htransform_and_likelihood, eltype(forward_kernels), typeof(h))\n    post_forward_kernels = Vector{KT}(undef, length(forward_kernels))\n\n    for m in eachindex(forward_kernels)\n        fw_kernel = forward_kernels[end-m+1]\n        post_fw_kernel, h = htransform_and_likelihood(fw_kernel, h)\n        post_forward_kernels[end-m+1] = post_fw_kernel\n\n        like = likelihoods[end-m]\n        h = compose(h, like)\n    end\n    post_init, loglike = posterior_and_loglike(init, h)\n    return post_init, post_forward_kernels, loglike\nend\n\nnothing # hide","category":"page"},{"location":"tutorials/hidden_markov_model/#Computing-the-a-posteriori-distribution-of-the-hidden-sequence","page":"Sampling and inference in Hidden Markov models","title":"Computing the a posteriori distribution of the hidden sequence","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"likes = [Likelihood(Kobs, y) for (Kobs, y) in zip(obs_kernels, ys)] # compute the likelihoods associated with the observations\npost_init, post_fw_kernels, loglike = backward_recursion(init, fw_kernels, likes)\n\nnothing # hide","category":"page"},{"location":"tutorials/hidden_markov_model/#Sampling-a-posteriori-hidden-sequences-and-plotting-them","page":"Sampling and inference in Hidden Markov models","title":"Sampling a posteriori hidden sequences and plotting them","text":"","category":"section"},{"location":"tutorials/hidden_markov_model/","page":"Sampling and inference in Hidden Markov models","title":"Sampling and inference in Hidden Markov models","text":"nsample = 10\nfor _ in 1:nsample\n    xs_post = sample(rng, post_init, post_fw_kernels)\n    Plots.scatter!(\n        hmm_plt,\n        eachindex(xs_post),\n        xs_post,\n        label = \"\",\n        color = \"blue\",\n        alpha = 0.1,\n        )\nend\nhmm_plt","category":"page"},{"location":"affinemaps/affinemaps/#Affine-maps","page":"Affine maps","title":"Affine maps","text":"","category":"section"},{"location":"affinemaps/affinemaps/","page":"Affine maps","title":"Affine maps","text":"An affine map is a function f given by","category":"page"},{"location":"affinemaps/affinemaps/","page":"Affine maps","title":"Affine maps","text":" f(x) = A x + b","category":"page"},{"location":"affinemaps/affinemaps/","page":"Affine maps","title":"Affine maps","text":"where A is the slope and b is the intercept. Different representations of affine maps are sometimes useful, as documented below.","category":"page"},{"location":"affinemaps/affinemaps/#Types","page":"Affine maps","title":"Types","text":"","category":"section"},{"location":"affinemaps/affinemaps/#MarkovKernels.AbstractAffineMap","page":"Affine maps","title":"MarkovKernels.AbstractAffineMap","text":"AbstractAffineMap{T<:Number}\n\nAbstract type for representing affine maps between vector spaces over the field determined by T.\n\n\n\n\n\n","category":"type"},{"location":"affinemaps/affinemaps/#MarkovKernels.AffineMap","page":"Affine maps","title":"MarkovKernels.AffineMap","text":"AffineMap{T,U,V}\n\nType for representing affine maps in the standard slope / intercept parametrisation.\n\n\n\n\n\n","category":"type"},{"location":"affinemaps/affinemaps/#MarkovKernels.LinearMap","page":"Affine maps","title":"MarkovKernels.LinearMap","text":"LinearMap{T,U}\n\nType for representing affine maps with zero intercept.\n\n\n\n\n\n","category":"type"},{"location":"affinemaps/affinemaps/#MarkovKernels.AffineCorrector","page":"Affine maps","title":"MarkovKernels.AffineCorrector","text":"AffineCorrector{T,U,V,S}\n\nType for representing affine correctors, i.e.,\n\nx ↦ b + A * (x -c).\n\n\n\n\n\n","category":"type"},{"location":"affinemaps/affinemaps/#Constructors","page":"Affine maps","title":"Constructors","text":"","category":"section"},{"location":"affinemaps/affinemaps/#MarkovKernels.AffineMap-Tuple{AbstractMatrix, AbstractVector}","page":"Affine maps","title":"MarkovKernels.AffineMap","text":"AffineMap(A, b)\n\nCreates an AffineMap with slope A and intercept b.\n\n\n\n\n\n","category":"method"},{"location":"affinemaps/affinemaps/#MarkovKernels.LinearMap-Tuple{AbstractMatrix}","page":"Affine maps","title":"MarkovKernels.LinearMap","text":"LinearMap(A::AbstractMatrix)\n\nCreates a LinearMap with slope A.\n\n\n\n\n\n","category":"method"},{"location":"affinemaps/affinemaps/#MarkovKernels.AffineCorrector-Tuple{AbstractMatrix, AbstractVector, AbstractVector}","page":"Affine maps","title":"MarkovKernels.AffineCorrector","text":"AffineCorrector(A, b, c)\n\nCreates an Affine Corrector with slope A and intercept b - A * c.\n\n\n\n\n\n","category":"method"},{"location":"affinemaps/affinemaps/#Basics","page":"Affine maps","title":"Basics","text":"","category":"section"},{"location":"affinemaps/affinemaps/#MarkovKernels.slope-Tuple{AffineMap}","page":"Affine maps","title":"MarkovKernels.slope","text":"slope(a::AbstractAffineMap)\n\nComputes the slope of a.\n\n\n\n\n\n","category":"method"},{"location":"affinemaps/affinemaps/#MarkovKernels.intercept-Tuple{AffineMap}","page":"Affine maps","title":"MarkovKernels.intercept","text":"intercept(a::AffineMap)\n\nComputes the intercept of a.\n\n\n\n\n\n","category":"method"},{"location":"affinemaps/affinemaps/#MarkovKernels.compose-Tuple{AbstractAffineMap, AbstractAffineMap}","page":"Affine maps","title":"MarkovKernels.compose","text":"compose(a2::AbstractAffineMap, a1::AbstractAffineMap)\n\nComputes the affine map a3 resulting from the composition a2 ∘ a1.\n\nSee also ∘\n\n\n\n\n\n","category":"method"},{"location":"affinemaps/affinemaps/#Base.:∘-Tuple{AbstractAffineMap, AbstractAffineMap}","page":"Affine maps","title":"Base.:∘","text":"∘(a2::AbstractAffineMap, a1::AbstractAffineMap)\n\nEquivalent to compose(a2::AbstractAffineMap, a1::AbstractAffineMap).\n\nSee also compose\n\n\n\n\n\n","category":"method"},{"location":"#MarkovKernels","page":"Home","title":"MarkovKernels","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Documentation for MarkovKernels.","category":"page"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"distributions/normal/#Normal","page":"Normal","title":"Normal","text":"","category":"section"},{"location":"distributions/normal/","page":"Normal","title":"Normal","text":"The standard parametrisation of the Normal distribution is given by","category":"page"},{"location":"distributions/normal/","page":"Normal","title":"Normal","text":"mathcalN(x  mu   Sigma )","category":"page"},{"location":"distributions/normal/","page":"Normal","title":"Normal","text":"where mu is the mean vector and Sigma is the covariance matrix. The exact expression for the probabiltiy density function depends on whether x is vector with real or complex values, both are supported. For real valued vectors the density function is given by","category":"page"},{"location":"distributions/normal/","page":"Normal","title":"Normal","text":"mathcalN(x  mu   Sigma ) = 2pi Sigma^-12 exp Big(  -frac12(x-mu)^* Sigma^-1 (x-mu)  Big)","category":"page"},{"location":"distributions/normal/","page":"Normal","title":"Normal","text":"whereas for complex valued vectors the density function is given by","category":"page"},{"location":"distributions/normal/","page":"Normal","title":"Normal","text":"mathcalN(x  mu   Sigma ) = pi Sigma^-1 exp Big(  -(x-mu)^* Sigma^-1 (x-mu)  Big)","category":"page"},{"location":"distributions/normal/#Types","page":"Normal","title":"Types","text":"","category":"section"},{"location":"distributions/normal/#MarkovKernels.AbstractNormal","page":"Normal","title":"MarkovKernels.AbstractNormal","text":"AbstractNormal{ST}\n\nAbstract type for representing Normal distributed random vectors taking values in ST.\n\n\n\n\n\n","category":"type"},{"location":"distributions/normal/#MarkovKernels.Normal","page":"Normal","title":"MarkovKernels.Normal","text":"Normal{ST,U,V}\n\nStandard mean vector / covariance matrix parametrization of the Normal distribution with sample type ST.\n\n\n\n\n\n","category":"type"},{"location":"distributions/normal/#Constructors","page":"Normal","title":"Constructors","text":"","category":"section"},{"location":"distributions/normal/#MarkovKernels.Normal-Tuple{AbstractVector, Any}","page":"Normal","title":"MarkovKernels.Normal","text":"Normal(μ, Σ)\n\nCreates a Normal distribution with mean μ and covariance Σ.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#Methods","page":"Normal","title":"Methods","text":"","category":"section"},{"location":"distributions/normal/#MarkovKernels.dim-Tuple{Normal}","page":"Normal","title":"MarkovKernels.dim","text":"dim(N::AbstractNormal)\n\nReturns the dimension of the Normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#Statistics.mean-Tuple{Normal}","page":"Normal","title":"Statistics.mean","text":"mean(N::AbstractNormal)\n\nComputes the mean vector of the Normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#Statistics.cov-Tuple{Normal}","page":"Normal","title":"Statistics.cov","text":"cov(N::AbstractNormal)\n\nComputes the covariance matrix of the Normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#MarkovKernels.covparam-Tuple{Normal}","page":"Normal","title":"MarkovKernels.covparam","text":"covparam(N::AbstractNormal)\n\nReturns the internal representation of the covariance matrix of the Normal distribution N. For computing the actual covariance matrix, use cov.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#Statistics.var-Tuple{AbstractNormal}","page":"Normal","title":"Statistics.var","text":"var(N::AbstractNormal)\n\nComputes the vector of marginal variances of the Normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#Statistics.std-Tuple{AbstractNormal}","page":"Normal","title":"Statistics.std","text":"std(N::AbstractNormal)\n\nComputes the vector of marginal standard deviations of the Normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#MarkovKernels.residual-Tuple{AbstractNormal, AbstractVector}","page":"Normal","title":"MarkovKernels.residual","text":"residual(N::AbstractNormal, x::AbstractVector)\n\nComputes the whitened residual associated with the Normal distribution N and observed vector x.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#MarkovKernels.entropy-Tuple{AbstractNormal}","page":"Normal","title":"MarkovKernels.entropy","text":"entropy(N::AbstractNormal)\n\nComputes the entropy of the Normal distribution N.\n\n\n\n\n\n","category":"method"},{"location":"distributions/normal/#MarkovKernels.kldivergence-Tuple{AbstractNormal, AbstractNormal}","page":"Normal","title":"MarkovKernels.kldivergence","text":"kldivergence(N1::AbstractNormal, N2::AbstractNormal)\n\nComputes the Kullback-Leibler divergence between the Normal distributions N1 and N2.\n\n\n\n\n\n","category":"method"}]
}
